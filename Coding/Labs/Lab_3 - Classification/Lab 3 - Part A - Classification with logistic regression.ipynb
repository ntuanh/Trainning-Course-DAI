{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 (Part A): Classification with logistic regression\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "__IMPORTANT__ \n",
    "Please complete this Jupyter Notebook file and upload it to blackboard __before 13 February 2020__.\n",
    "</div>\n",
    "\n",
    "In this Lab, you will start by implementing logistic regression (for classification) and apply it to a dataset. Before starting, make sure that you read the slides of lecture 3.\n",
    "\n",
    "Your task in the first part of this Lab will be to build a logistic regression model to predict whether a student gets admitted into a university.\n",
    "\n",
    "Suppose that you are the administrator of a university department and you want to determine each applicant's chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for logistic regression. For each training example, you have the applicant's scores on two exams and the admissions decision. You will build a classification model that estimates an applicant's probability of admission based on the scores from those two exams.\n",
    "\n",
    "## Loading the data\n",
    "We have a file `university-admission-dataset.csv` which contains the dataset for our classification problem. The first column is the score obtained at exam1, the second column is the score obtained at exam2, and the third column is the class-label indicating if the student has been admitted or not (1 = Admitted, 0 = Not admitted).\n",
    "\n",
    "<img src=\"imgs/UnivAdmDataLab3A.png\" />\n",
    "\n",
    "The following Python code helps you load the dataset from the csv file into the variables $X$ and $y$. The variable $X$ is the input data, which is a matrix with two columns (two feature) corresponding to the score at exam1 and the score at exam2. The variable $y$ is the output class-labels corresponding to whether or not each student has been admitted. Read the following code and print a small subset of $X$ and $y$ to see what they look like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "(100, 3)\n",
      "(100, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' TODO:\\nYou can print here a small subset of X and y (e.g. corresponding to 10 students) to see what they look like.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "\n",
    "# Loading the data from the file into mydata\n",
    "filename = \"datasets/university-admission-dataset.csv\"\n",
    "mydata = np.genfromtxt(filename, delimiter=\",\")\n",
    "\n",
    "# We have n students (each line corresponds to one student)\n",
    "n = len(mydata)\n",
    "print(n)\n",
    "\n",
    "# We take the two first columns from mydata. So, X is a matrix of n lines and two \n",
    "# columns (\"score at exam1\" and \"score at exam2\"), i.e. an array of n 2-dimensional data-points\n",
    "X = mydata[:, :2]\n",
    "print(mydata.shape)\n",
    "print(X.shape)\n",
    "\n",
    "# We take the class-labels from mydata (-1 refers to the last column)\n",
    "# So, y is the vector of outputs, i.e. an array of n scalar values\n",
    "y = mydata[:, -1]\n",
    "\n",
    "\"\"\" TODO:\n",
    "You can print here a small subset of X and y (e.g. corresponding to 10 students) to see what they look like.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data\n",
    "Before starting to implement any learning algorithm, it is always good to visualize the data if possible. Complete the following Python code so that it displays a figure like the one shown below. The axes are the two exam scores (i.e. our features), and the class-labels are shown with different markers/colors.\n",
    "<img src=\"imgs/UnivAdmScatterPlotLab3A.png\" width=\"500px\" />\n",
    "\n",
    "**Practical hint:**\n",
    "Suppose that we have two numpy arrays `a` and `b` of the same length, as in the following example:\n",
    "```python\n",
    "a = np.array([\"This\", \"is\", \"an\", \"interesting\", \"example\"])\n",
    "b = np.array([\"yes\", \"yes\", \"no\", \"yes\", \"no\"])\n",
    "```\n",
    "If we write `b == \"yes\"`, this will produce a boolean array: `[True, True, False, True, False]`. This array contains `True` if the corresponding value in `b` is `\"yes\"`, and `\"False\"` otherwise. Similarly, `b == \"no\"` will produce the boolean array `[False, False,  True, False,  True]`. Moreover, given an array of boolean values, e.g. `arr = [True,  True, False,  True, False]`, if we write `a[arr]` then we get `[\"This\", \"is\", \"interesting\"]`. This corresponds to the elements of `a` where the corresponding value in `arr` was `True`. In summary, to get the elements of a where the corresponding value in b is \"yes\", we can simply use `a[b == \"yes\"]`.\n",
    "```python\n",
    "print( a[b == \"yes\"] ) # This gives: [\"This\", \"is\", \"interesting\"]\n",
    "print( a[b == \"no\"] ) # This gives: [\"an\", \"example\"]\n",
    "```\n",
    "You can use this syntax to select a subset of data-points (from $X$) which have a given label (in $y$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGiCAYAAADA0E3hAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHMNJREFUeJzt3W9s3VX9wPFP29FbCLRM59ptFisoogIbbqwWJIipNoFM98A4wWxz4Y/gJLhGZWOwiug6EciiKy5MEB+omxAwxi1DrC4GqVnY1gRkg8DATWMLE9fOIi1rv78Hhvqr62C39M9O+3ol98GO59zvuR5G39x/LciyLAsAgAQUjvUGAACOlXABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkpF3uPzhD3+IefPmxfTp06OgoCB++ctfvuWabdu2xUc+8pHI5XLxvve9L+6///4hbBUAmOjyDpeurq6YOXNmNDU1HdP8F154IS677LK45JJLorW1Nb761a/GVVddFY888kjemwUAJraCt/NLFgsKCuLhhx+O+fPnH3XOjTfeGJs3b46nnnqqf+zzn/98HDx4MLZu3TrUSwMAE9Ckkb5AS0tL1NbWDhirq6uLr371q0dd093dHd3d3f1/7uvri1deeSXe+c53RkFBwUhtFQAYRlmWxaFDh2L69OlRWDg8b6sd8XBpa2uL8vLyAWPl5eXR2dkZ//73v+PEE088Yk1jY2PceuutI701AGAU7N+/P9797ncPy32NeLgMxYoVK6K+vr7/zx0dHXHaaafF/v37o7S0dAx3BgAcq87OzqisrIxTTjll2O5zxMOloqIi2tvbB4y1t7dHaWnpoM+2RETkcrnI5XJHjJeWlgoXAEjMcL7NY8S/x6Wmpiaam5sHjD366KNRU1Mz0pcGAMaZvMPlX//6V7S2tkZra2tE/Ofjzq2trbFv376I+M/LPIsWLeqff+2118bevXvjG9/4RuzZsyfuvvvu+MUvfhHLli0bnkcAAEwYeYfLE088Eeedd16cd955ERFRX18f5513XqxatSoiIv7+97/3R0xExHvf+97YvHlzPProozFz5sy4884740c/+lHU1dUN00MAACaKt/U9LqOls7MzysrKoqOjw3tcACARI/Hz2+8qAgCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGUMKl6ampqiqqoqSkpKorq6O7du3v+n8tWvXxgc+8IE48cQTo7KyMpYtWxavvfbakDYMAExceYfLpk2bor6+PhoaGmLnzp0xc+bMqKuri5deemnQ+T/72c9i+fLl0dDQELt374577703Nm3aFDfddNPb3jwAMLHkHS533XVXXH311bFkyZL40Ic+FOvXr4+TTjop7rvvvkHnP/7443HhhRfGFVdcEVVVVfGpT30qLr/88rd8lgYA4H/lFS49PT2xY8eOqK2t/e8dFBZGbW1ttLS0DLrmggsuiB07dvSHyt69e2PLli1x6aWXHvU63d3d0dnZOeAGADApn8kHDhyI3t7eKC8vHzBeXl4ee/bsGXTNFVdcEQcOHIiPfexjkWVZHD58OK699to3famosbExbr311ny2BgBMACP+qaJt27bF6tWr4+67746dO3fGQw89FJs3b47bbrvtqGtWrFgRHR0d/bf9+/eP9DYBgATk9YzLlClToqioKNrb2weMt7e3R0VFxaBrbrnllli4cGFcddVVERFxzjnnRFdXV1xzzTWxcuXKKCw8sp1yuVzkcrl8tgYATAB5PeNSXFwcs2fPjubm5v6xvr6+aG5ujpqamkHXvPrqq0fESVFRUUREZFmW734BgAksr2dcIiLq6+tj8eLFMWfOnJg7d26sXbs2urq6YsmSJRERsWjRopgxY0Y0NjZGRMS8efPirrvuivPOOy+qq6vjueeei1tuuSXmzZvXHzAAAMci73BZsGBBvPzyy7Fq1apoa2uLWbNmxdatW/vfsLtv374Bz7DcfPPNUVBQEDfffHP87W9/i3e9610xb968+M53vjN8jwIAmBAKsgRer+ns7IyysrLo6OiI0tLSsd4OAHAMRuLnt99VBAAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoYULk1NTVFVVRUlJSVRXV0d27dvf9P5Bw8ejKVLl8a0adMil8vFmWeeGVu2bBnShgGAiWtSvgs2bdoU9fX1sX79+qiuro61a9dGXV1dPPPMMzF16tQj5vf09MQnP/nJmDp1ajz44IMxY8aM+Mtf/hKnnnrqcOwfAJhACrIsy/JZUF1dHeeff36sW7cuIiL6+vqisrIyrr/++li+fPkR89evXx/f+973Ys+ePXHCCScMaZOdnZ1RVlYWHR0dUVpaOqT7AABG10j8/M7rpaKenp7YsWNH1NbW/vcOCgujtrY2WlpaBl3zq1/9KmpqamLp0qVRXl4eZ599dqxevTp6e3uPep3u7u7o7OwccAMAyCtcDhw4EL29vVFeXj5gvLy8PNra2gZds3fv3njwwQejt7c3tmzZErfcckvceeed8e1vf/uo12lsbIyysrL+W2VlZT7bBADGqRH/VFFfX19MnTo17rnnnpg9e3YsWLAgVq5cGevXrz/qmhUrVkRHR0f/bf/+/SO9TQAgAXm9OXfKlClRVFQU7e3tA8bb29ujoqJi0DXTpk2LE044IYqKivrHPvjBD0ZbW1v09PREcXHxEWtyuVzkcrl8tgYATAB5PeNSXFwcs2fPjubm5v6xvr6+aG5ujpqamkHXXHjhhfHcc89FX19f/9izzz4b06ZNGzRaAACOJu+Xiurr62PDhg3xk5/8JHbv3h3XXXdddHV1xZIlSyIiYtGiRbFixYr++dddd1288sorccMNN8Szzz4bmzdvjtWrV8fSpUuH71EAABNC3t/jsmDBgnj55Zdj1apV0dbWFrNmzYqtW7f2v2F33759UVj43x6qrKyMRx55JJYtWxbnnntuzJgxI2644Ya48cYbh+9RAAATQt7f4zIWfI8LAKRnzL/HBQBgLAkXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASMaQwqWpqSmqqqqipKQkqqurY/v27ce0buPGjVFQUBDz588fymUBgAku73DZtGlT1NfXR0NDQ+zcuTNmzpwZdXV18dJLL73puhdffDG+9rWvxUUXXTTkzQIAE1ve4XLXXXfF1VdfHUuWLIkPfehDsX79+jjppJPivvvuO+qa3t7e+MIXvhC33nprnH766W95je7u7ujs7BxwAwDIK1x6enpix44dUVtb+987KCyM2traaGlpOeq6b33rWzF16tS48sorj+k6jY2NUVZW1n+rrKzMZ5sAwDiVV7gcOHAgent7o7y8fMB4eXl5tLW1Dbrmsccei3vvvTc2bNhwzNdZsWJFdHR09N/279+fzzYBgHFq0kje+aFDh2LhwoWxYcOGmDJlyjGvy+VykcvlRnBnAECK8gqXKVOmRFFRUbS3tw8Yb29vj4qKiiPmP//88/Hiiy/GvHnz+sf6+vr+c+FJk+KZZ56JM844Yyj7BgAmoLxeKiouLo7Zs2dHc3Nz/1hfX180NzdHTU3NEfPPOuusePLJJ6O1tbX/9ulPfzouueSSaG1t9d4VACAveb9UVF9fH4sXL445c+bE3LlzY+3atdHV1RVLliyJiIhFixbFjBkzorGxMUpKSuLss88esP7UU0+NiDhiHADgreQdLgsWLIiXX345Vq1aFW1tbTFr1qzYunVr/xt29+3bF4WFvpAXABh+BVmWZWO9ibfS2dkZZWVl0dHREaWlpWO9HQDgGIzEz29PjQAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkIwhhUtTU1NUVVVFSUlJVFdXx/bt2486d8OGDXHRRRfF5MmTY/LkyVFbW/um8wEAjibvcNm0aVPU19dHQ0ND7Ny5M2bOnBl1dXXx0ksvDTp/27Ztcfnll8fvf//7aGlpicrKyvjUpz4Vf/vb39725gGAiaUgy7IsnwXV1dVx/vnnx7p16yIioq+vLyorK+P666+P5cuXv+X63t7emDx5cqxbty4WLVo06Jzu7u7o7u7u/3NnZ2dUVlZGR0dHlJaW5rNdAGCMdHZ2RllZ2bD+/M7rGZeenp7YsWNH1NbW/vcOCgujtrY2Wlpajuk+Xn311Xj99dfjHe94x1HnNDY2RllZWf+tsrIyn20CAONUXuFy4MCB6O3tjfLy8gHj5eXl0dbWdkz3ceONN8b06dMHxM//WrFiRXR0dPTf9u/fn882AYBxatJoXmzNmjWxcePG2LZtW5SUlBx1Xi6Xi1wuN4o7AwBSkFe4TJkyJYqKiqK9vX3AeHt7e1RUVLzp2jvuuCPWrFkTv/3tb+Pcc8/Nf6cAwISX10tFxcXFMXv27Ghubu4f6+vri+bm5qipqTnquttvvz1uu+222Lp1a8yZM2fouwUAJrS8Xyqqr6+PxYsXx5w5c2Lu3Lmxdu3a6OrqiiVLlkRExKJFi2LGjBnR2NgYERHf/e53Y9WqVfGzn/0sqqqq+t8Lc/LJJ8fJJ588jA8FABjv8g6XBQsWxMsvvxyrVq2Ktra2mDVrVmzdurX/Dbv79u2LwsL/PpHzwx/+MHp6euKzn/3sgPtpaGiIb37zm29v9wDAhJL397iMhZH4HDgAMLLG/HtcAADGknABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAwpXJqamqKqqipKSkqiuro6tm/f/qbzH3jggTjrrLOipKQkzjnnnNiyZcuQNgsATGx5h8umTZuivr4+GhoaYufOnTFz5syoq6uLl156adD5jz/+eFx++eVx5ZVXxq5du2L+/Pkxf/78eOqpp9725gGAiaUgy7IsnwXV1dVx/vnnx7p16yIioq+vLyorK+P666+P5cuXHzF/wYIF0dXVFb/+9a/7xz760Y/GrFmzYv369YNeo7u7O7q7u/v/3NHREaeddlrs378/SktL89kuADBGOjs7o7KyMg4ePBhlZWXDcp+T8pnc09MTO3bsiBUrVvSPFRYWRm1tbbS0tAy6pqWlJerr6weM1dXVxS9/+cujXqexsTFuvfXWI8YrKyvz2S4AcBz4xz/+MTbhcuDAgejt7Y3y8vIB4+Xl5bFnz55B17S1tQ06v62t7ajXWbFixYDYOXjwYLznPe+Jffv2DdsDZ2jeqGfPfo09Z3H8cBbHF+dx/HjjFZN3vOMdw3afeYXLaMnlcpHL5Y4YLysr8w/hcaK0tNRZHCecxfHDWRxfnMfxo7Bw+D7EnNc9TZkyJYqKiqK9vX3AeHt7e1RUVAy6pqKiIq/5AABHk1e4FBcXx+zZs6O5ubl/rK+vL5qbm6OmpmbQNTU1NQPmR0Q8+uijR50PAHA0eb9UVF9fH4sXL445c+bE3LlzY+3atdHV1RVLliyJiIhFixbFjBkzorGxMSIibrjhhrj44ovjzjvvjMsuuyw2btwYTzzxRNxzzz3HfM1cLhcNDQ2DvnzE6HIWxw9ncfxwFscX53H8GImzyPvj0BER69ati+9973vR1tYWs2bNiu9///tRXV0dEREf//jHo6qqKu6///7++Q888EDcfPPN8eKLL8b73//+uP322+PSSy8dtgcBAEwMQwoXAICx4HcVAQDJEC4AQDKECwCQDOECACTjuAmXpqamqKqqipKSkqiuro7t27e/6fwHHnggzjrrrCgpKYlzzjkntmzZMko7Hf/yOYsNGzbERRddFJMnT47JkydHbW3tW54dxy7fvxdv2LhxYxQUFMT8+fNHdoMTSL5ncfDgwVi6dGlMmzYtcrlcnHnmmf49NUzyPYu1a9fGBz7wgTjxxBOjsrIyli1bFq+99too7Xb8+sMf/hDz5s2L6dOnR0FBwZv+DsI3bNu2LT7ykY9ELpeL973vfQM+gXzMsuPAxo0bs+Li4uy+++7L/vznP2dXX311duqpp2bt7e2Dzv/jH/+YFRUVZbfffnv29NNPZzfffHN2wgknZE8++eQo73z8yfcsrrjiiqypqSnbtWtXtnv37uyLX/xiVlZWlv31r38d5Z2PP/mexRteeOGFbMaMGdlFF12UfeYznxmdzY5z+Z5Fd3d3NmfOnOzSSy/NHnvsseyFF17Itm3blrW2to7yzseffM/ipz/9aZbL5bKf/vSn2QsvvJA98sgj2bRp07Jly5aN8s7Hny1btmQrV67MHnrooSwisocffvhN5+/duzc76aSTsvr6+uzpp5/OfvCDH2RFRUXZ1q1b87rucREuc+fOzZYuXdr/597e3mz69OlZY2PjoPM/97nPZZdddtmAserq6uxLX/rSiO5zIsj3LP7X4cOHs1NOOSX7yU9+MlJbnDCGchaHDx/OLrjgguxHP/pRtnjxYuEyTPI9ix/+8IfZ6aefnvX09IzWFieMfM9i6dKl2Sc+8YkBY/X19dmFF144ovucaI4lXL7xjW9kH/7whweMLViwIKurq8vrWmP+UlFPT0/s2LEjamtr+8cKCwujtrY2WlpaBl3T0tIyYH5ERF1d3VHnc2yGchb/69VXX43XX399WH8T6EQ01LP41re+FVOnTo0rr7xyNLY5IQzlLH71q19FTU1NLF26NMrLy+Pss8+O1atXR29v72hte1wayllccMEFsWPHjv6Xk/bu3RtbtmzxJahjYLh+do/5b4c+cOBA9Pb2Rnl5+YDx8vLy2LNnz6Br2traBp3f1tY2YvucCIZyFv/rxhtvjOnTpx/xDyf5GcpZPPbYY3HvvfdGa2vrKOxw4hjKWezduzd+97vfxRe+8IXYsmVLPPfcc/HlL385Xn/99WhoaBiNbY9LQzmLK664Ig4cOBAf+9jHIsuyOHz4cFx77bVx0003jcaW+X+O9rO7s7Mz/v3vf8eJJ554TPcz5s+4MH6sWbMmNm7cGA8//HCUlJSM9XYmlEOHDsXChQtjw4YNMWXKlLHezoTX19cXU6dOjXvuuSdmz54dCxYsiJUrV8b69evHemsTzrZt22L16tVx9913x86dO+Ohhx6KzZs3x2233TbWW2OIxvwZlylTpkRRUVG0t7cPGG9vb4+KiopB11RUVOQ1n2MzlLN4wx133BFr1qyJ3/72t3HuueeO5DYnhHzP4vnnn48XX3wx5s2b1z/W19cXERGTJk2KZ555Js4444yR3fQ4NZS/F9OmTYsTTjghioqK+sc++MEPRltbW/T09ERxcfGI7nm8GspZ3HLLLbFw4cK46qqrIiLinHPOia6urrjmmmti5cqVUVjov99Hy9F+dpeWlh7zsy0Rx8EzLsXFxTF79uxobm7uH+vr64vm5uaoqakZdE1NTc2A+RERjz766FHnc2yGchYREbfffnvcdtttsXXr1pgzZ85obHXcy/cszjrrrHjyySejtbW1//bpT386LrnkkmhtbY3KysrR3P64MpS/FxdeeGE899xz/fEYEfHss8/GtGnTRMvbMJSzePXVV4+IkzeCMvOr+kbVsP3szu99wyNj48aNWS6Xy+6///7s6aefzq655prs1FNPzdra2rIsy7KFCxdmy5cv75//xz/+MZs0aVJ2xx13ZLt3784aGhp8HHqY5HsWa9asyYqLi7MHH3ww+/vf/95/O3To0Fg9hHEj37P4Xz5VNHzyPYt9+/Zlp5xySvaVr3wle+aZZ7Jf//rX2dSpU7Nvf/vbY/UQxo18z6KhoSE75ZRTsp///OfZ3r17s9/85jfZGWeckX3uc58bq4cwbhw6dCjbtWtXtmvXriwisrvuuivbtWtX9pe//CXLsixbvnx5tnDhwv75b3wc+utf/3q2e/furKmpKd2PQ2dZlv3gBz/ITjvttKy4uDibO3du9qc//an/f7v44ouzxYsXD5j/i1/8IjvzzDOz4uLi7MMf/nC2efPmUd7x+JXPWbznPe/JIuKIW0NDw+hvfBzK9+/F/ydchle+Z/H4449n1dXVWS6Xy04//fTsO9/5Tnb48OFR3vX4lM9ZvP7669k3v/nN7IwzzshKSkqyysrK7Mtf/nL2z3/+c/Q3Ps78/ve/H/Tf/2/8/7948eLs4osvPmLNrFmzsuLi4uz000/PfvzjH+d93YIs81wZAJCGMX+PCwDAsRIuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQjP8DPZCkbwFa2SAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\"\"\" TODO:\n",
    "Follow the steps below to complete the code and produce a scatter \n",
    "plot of the training data like the one shown in the figure above.\n",
    "\"\"\"\n",
    "\n",
    "# TODO: select the data-points from X whose corresponding class-label is 0 (not admitted).\n",
    " \n",
    "# TODO: select the data-points from X whose corresponding class-label is 1 (admitted).\n",
    " \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# TODO: scatter plot for the non admitted students (X0)\n",
    "# TODO: scatter plot for the admitted students (X1)\n",
    "# TODO: set the 1st axis label to \"Exam 1 score\"\n",
    "# TODO: set the 2nd axis label to \"Exam 2 score\"\n",
    "# TODO: set the title to \"Scatter plot of training data\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a first column of ones to the dataset\n",
    "Before starting the implementation of logistic regression, it might be helpful to use a modified version of our dataset which has an additional first column of ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TODO:\\nYou can print a small subset of X_new here to see how it looks like \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function takes a matrix as argument and returns a new matrix with an additional first column (of ones)\n",
    "def add_all_ones_column(X):\n",
    "    n, d = X.shape # dimension of the matrix X (n lines, d columns)\n",
    "    XX = np.ones((n, d+1)) # new matrix of all ones with one additional column\n",
    "    XX[:, 1:] = X # set X starting from column 1 (keep only column 0 unchanged)\n",
    "    return XX\n",
    "\n",
    "# The following line creates a new data matrix X_new with an additional first column (of ones)\n",
    "X_new = add_all_ones_column(X)\n",
    "\n",
    "\"\"\" TODO:\n",
    "You can print a small subset of X_new here to see how it looks like \n",
    "\"\"\"\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid function\n",
    "Before you start implementing the actual cost function, recall that the logistic regression hypothesis is defined as:\n",
    "$$h_\\theta(x) = g(\\theta^T x),$$\n",
    "where function $g$ is the `sigmoid` function, defined as:\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Your first step is to complete the code below to implement this function so it can be called by the rest of your program. When you are finished, try testing a few values by calling `sigmoid(z)`. For large positive values of x, the sigmoid should be close to $1$, while for large negative values, the sigmoid should be close to $0$. Evaluating `sigmoid(0)` should give you exactly $0.5$. Your code should also work with vectors and matrices. For a matrix, your function should perform the sigmoid function on every element. You can use the numpy function `np.exp(..)` to compute the exponancial. This later works on scalar values, vectors as well as matricies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TODO (OPTIONAL):\\nThe above hypothesis function h(theta, x) makes a prediction for only one data-point x.\\nWrite here a modified function h_all(theta, X) which gives predictions for a dataset.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" TODO:\n",
    "Write code for the definition of the sigmoid function. This function \n",
    "should work on a scalar value as well as a vector or a matrix.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Test your sigmoid function by calling it few times on some scalar values, a vector, and then matrix.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# The hypothesis function is defined as follows:\n",
    "def h(theta, x):\n",
    "    return sigmoid(theta.T @ x)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" TODO (OPTIONAL):\n",
    "The above hypothesis function h(theta, x) makes a prediction for only one data-point x.\n",
    "Write here a modified function h_all(theta, X) which gives predictions for a dataset.\n",
    "\"\"\"\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function and gradient\n",
    "Now you will implement the cost function and gradient for logistic regression. Recall that the cost function in logistic regression is:\n",
    "$$E(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} \\left [ -y^{(i)} \\log{(h_\\theta(x^{(i)}))} - (1 - y^{(i)}) \\log{(1 - h_\\theta(x^{(i)}))} \\right ]$$\n",
    "\n",
    "and the gradient of the cost, $\\nabla E(\\theta)$, is a vector of the same length as $\\theta$ defined as:\n",
    "\n",
    "$$\n",
    "\\nabla E(\\theta) = \\left ( \\frac{\\partial E(\\theta)}{\\partial \\theta_0}, \\frac{\\partial E(\\theta)}{\\partial \\theta_1}, \\frac{\\partial E(\\theta)}{\\partial \\theta_2}, \\dots \\right )\n",
    "\\quad \\quad \\text{ where the $j^{th}$ element }\n",
    "\\frac{\\partial E(\\theta)}{\\partial \\theta_j} = \\frac{1}{n} \\sum_{i=1}^{n} \\left [ h_{\\theta}(x^{(i)}) - y^{(i)} \\right ] ~ x^{(i)}_j\n",
    "$$\n",
    "\n",
    "Note that while this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of $h_\\theta(x)$.\n",
    "\n",
    "Complete the Python code below to return the cost and gradient. You can use `np.log(..)` to compute the $\\log$. Once you implement the cost function $E$ correctly, calling it with an initial $\\theta$ of zeros should return a cost of about $0.693$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TODO:\\nWrite the definition of the gradient function. It should return an array containing \\nthe derivative of the cost function with respect to each parameter theta[j].\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" TODO:\n",
    "Write code for the definition of the cost function. If possible, try to implement \n",
    "it in a vectorized form (by manipulating arrays directly without using a loop). If \n",
    "you are not able to implement it in a vectorized form, then it's fine to use a loop.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Call your cost function here to test it using an initial theta of all zeros, and \n",
    "using X_new (instead of X). Calling E(theta, X_new, y) should return about 0.693.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Write the definition of the gradient function. It should return an array containing \n",
    "the derivative of the cost function with respect to each parameter theta[j].\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning parameters using scipy.optimize.minimize\n",
    "\n",
    "In the previous Lab, you found the optimal parameters of a linear regression model by implementing gradient descent. You wrote a cost function and calculated its gradient, then took a gradient descent step accordingly. This time, instead of taking gradient descent steps, you will use a Python function predefined in the `scipy` library: `scipy.optimize.minimize(..)`. This function is an optimization solver that finds the minimum of a given function. For logistic regression, you want to optimize the cost function $E(\\theta)$ with parameters $\\theta$.\n",
    "\n",
    "Concretely, you are going to use `scipy.optimize.minimize(..)` to find the best parameters $\\theta$ for the logistic regression cost function, given a fixed dataset (`X_new` and `y` values). You will pass to `scipy.optimize.minimize(..)` the following arguments (in that order) :\n",
    "\n",
    "- Name of the cost function to be minimized. In our case, it's just `E`\n",
    "- Array of initial parameter values $\\theta$. In our case, `theta`.\n",
    "- Tuple of extra arguments (in addition to $\\theta$) passed to the cost function and the gradient function. In our case, it's `(X_new, y)`\n",
    "- String corresponding to type of the optimization solver. Here you can use `\"TNC\"` which refers to *Truncated Newton algorithm*.\n",
    "- Name of the function that computes the gradient vector. In our case, it's just `gradE`.\n",
    "\n",
    "`scipy.optimize.minimize(..)` returns an object containing the optimization results. The most important attribute of this object is called `x` and represent the optimal solution array (i.e. the best $\\theta$). So, once you call `res = scipy.optimize.minimize(..)`, you can get the optimal $\\theta$ simply using `res.x`.\n",
    "\n",
    "For more information about `scipy.optimize.minimize(..)`, you can visit:\n",
    "- https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize\n",
    "- https://docs.scipy.org/doc/scipy/reference/optimize.html\n",
    "\n",
    "Complete the Python code below to find the optimal parameter vector $\\theta$ for logistic regression.\n",
    "\n",
    "Notice that by using `scipy.optimize.minimize(..)`, you do not have to write any loops yourself, or set a learning rate like you did for gradient descent. This is all done by `scipy.optimize.minimize(..)`: you only needed to provide a function calculating the cost and the gradient. Once `scipy.optimize.minimize(..)` completes, calling the cost function function using the optimal parameters of $\\theta$  should give you a cost of about $0.203$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TODO:\\nUse op.minimize(..) with the right arguments (as explained above) in order \\nto minimize the cost function E. Then, print the optimal parameter vector theta and the final cost.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.optimize as op\n",
    "\n",
    "theta = np.array([0, 0, 0])  # Some initial parameters vector\n",
    "# print(\"Initial cost: \", E(theta, X_new, y))\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Use op.minimize(..) with the right arguments (as explained above) in order \n",
    "to minimize the cost function E. Then, print the optimal parameter vector theta and the final cost.\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the decision boundary\n",
    "One you get the optimal parameters of $\\theta$, you can call the function `plot_decision_boundary(X, y, theta)` defined in the code below to plot the original dataset and the decision boundary. Read the code carefully to see how such a boundary is ploted using the $\\theta$ values. Basically, the equation of our decision boundary is: $\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 = 0$. So, to plot it, we generate a range of values for $x_1$ and we compute the corresponding values of $x_2 = - \\frac{\\theta_0 + \\theta_1 x_1}{\\theta_2}$.\n",
    "\n",
    "You are supposed to get a plot that looks like the following figure:\n",
    "<img src=\"imgs/UnivAdmScatterPlotLab3AwithDB.png\" width=\"500px\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TODO:\\nCall the function plot_decision_boundary(X, y, theta) with \\nthe optimal theta parameters that you got previously.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# This is a function that plots the original dataset (X, y) and decision boundary:\n",
    "def plot_decision_boundary(X, y, theta):\n",
    "    X0 = X[y==0] # subset of the non admitted students\n",
    "    X1 = X[y==1] # subset of the admitted students\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Plottin the dataset:\n",
    "    ax.scatter(X0[:, 0], X0[:, 1], marker=\"o\", color=\"red\", label=\"Non admitted\")\n",
    "    ax.scatter(X1[:, 0], X1[:, 1], marker=\"*\", color=\"blue\", label=\"Admitted\")\n",
    "    ax.set_xlabel(\"Exam 1 score\")\n",
    "    ax.set_ylabel(\"Exam 2 score\")\n",
    "    \n",
    "    # Plotting the decision boundary:\n",
    "    plot_x1 = np.arange(30, 100) # range of values from 30 to 100\n",
    "    plot_x2 = - (theta[0] + theta[1] * plot_x1) / theta[2]\n",
    "    print(f\"Check out plot 2: {plot_x2}\")\n",
    "    print(f\"Check out theta[2] :{theta[2]}\")\n",
    "    plt.plot(plot_x1, plot_x2, color=\"green\", label=\"Decision boundary\")\n",
    "    \n",
    "    ax.set_title(\"Plot of the training data and decision boundary\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\"\"\" TODO:\n",
    "Call the function plot_decision_boundary(X, y, theta) with \n",
    "the optimal theta parameters that you got previously.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the logistic regression model\n",
    "After learning the parameters, you can use the model to predict whether a particular student will be admitted. For a student with an Exam 1 score of $45$ and an Exam 2 score of $85$, you should expect to see an admission probability of about $0.776$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TODO:\\nPredict the admission probability of student x, by calling the hypothesis \\nfunction with x and the optimal parameters theta. You should expect to get \\nan admission probability of about 0.776 for this student x.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Student with an Exam 1 score of 45 and an Exam 2 score of 85\n",
    "x = np.array([1, 45, 85])\n",
    "\n",
    "\"\"\" TODO:\n",
    "Predict the admission probability of student x, by calling the hypothesis \n",
    "function with x and the optimal parameters theta. You should expect to get \n",
    "an admission probability of about 0.776 for this student x.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to evaluate the quality of the parameters we have found is to see how well the learned model predicts on our training set. In this part, your task is to complete the Python code below (read the *TODO* comments carefully). The `predict` function should produce an array of \"$1$\" or \"$0$\" predictions given a dataset and a learned parameters vector $\\theta$. After you have completed the implementation of the `predict` function, you are asked to report the training accuracy of your classifier by computing the percentage of examples for which you correctly predicted the class-label.\n",
    "\n",
    "*Note*: We will see later in the course that computing the ***training** accuracy* is NOT a good way to evaluate the quality of your machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TODO:\\nPredict the class-labels of the data-points in the training set by calling the function \\npredict(..) with the optimal parameters vector theta and X_new. Then, compute the classification \\naccuracy by comparing the predicted class-labels with the actual (true) class-labels y.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "\"\"\" TODO:\n",
    "Write the definition of the function predict(theta, X) which returns an array of \n",
    "predictions. Each prediction corresponds to a data-point x and is either 1 or 0, \n",
    "depending on whether or not the admission probability h(theta, x) is higher than 0.5.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" TODO:\n",
    "Predict the class-labels of the data-points in the training set by calling the function \n",
    "predict(..) with the optimal parameters vector theta and X_new. Then, compute the classification \n",
    "accuracy by comparing the predicted class-labels with the actual (true) class-labels y.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
